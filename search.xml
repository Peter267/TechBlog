<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>RSS是个甚么东东</title>
    <url>/2024/07/26/RSS%E6%98%AF%E4%B8%AA%E7%94%9A%E4%B9%88%E4%B8%9C%E4%B8%9C/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>大家在浏览独立博客网站时可能会发现，有的网站内有个类似于手机WI-FI标志的按钮，这其实是RSS（简易信息聚合）订阅。以<a href="https://[peter267.github.io//rss.xml](https://peter267.github.io//rss.xml)">本站</a>为例，它左上角有个RSS订阅功能，当我们点击后，会得到一个格式为xml格式的文件。那么，RSS是什么，为什么，怎么用呢</p>
<h1 id="是什么"><a href="#是什么" class="headerlink" title="是什么"></a>是什么</h1><blockquote>
<p>引用自<a href="https://baike.baidu.com/item/RSS/24470">百度百科</a><br><a href="https://baike.baidu.com/item/%E7%AE%80%E6%98%93%E4%BF%A1%E6%81%AF%E8%81%9A%E5%90%88/6453727?fromModule=lemma_inlink">简易信息聚合</a>（也叫聚合内容）是一种基于XML的标准，在互联网上被广泛采用的内容包装和投递协议。RSS(Really Simple Syndication)是一种描述和同步网站内容的格式，是使用最广泛的XML应用。RSS搭建了信息迅速传播的一个技术平台，使得每个人都成为潜在的<a href="https://baike.baidu.com/item/%E4%BF%A1%E6%81%AF%E6%8F%90%E4%BE%9B%E8%80%85/12754057?fromModule=lemma_inlink">信息提供者</a>。发布一个RSS文件后，这个RSS <a href="https://baike.baidu.com/item/Feed/15181?fromModule=lemma_inlink">Feed</a>中包含的信息就能直接被其他站点调用，而且由于这些数据都是标准的XML格式，所以也能在其他的终端和服务中使用，是一种描述和同步<a href="https://baike.baidu.com/item/%E7%BD%91%E7%AB%99%E5%86%85%E5%AE%B9/6694752?fromModule=lemma_inlink">网站内容</a>的格式。 [1]RSS可以是以下三个解释的其中一个： Really Simple Syndication；RDF (Resource Description Framework) Site Summary； Rich Site Summary。但其实这三个解释都是指同一种Syndication的技术。<br>RSS广泛用于网上新闻频道，<a href="https://baike.baidu.com/item/blog/8086465?fromModule=lemma_inlink">blog</a>和<a href="https://baike.baidu.com/item/wiki/97755?fromModule=lemma_inlink">wiki</a>，主要的版本有0.91, 1.0, 2.0。使用<a href="https://baike.baidu.com/item/RSS%E8%AE%A2%E9%98%85/663114?fromModule=lemma_inlink">RSS订阅</a><strong>能更快地获取信息</strong>，网站提供RSS输出，有<strong>利于让用户获取网站内容的最新更新</strong>。<a href="https://baike.baidu.com/item/%E7%BD%91%E7%BB%9C%E7%94%A8%E6%88%B7/56340765?fromModule=lemma_inlink">网络用户</a>可以在客户端借助于支持RSS的聚合工具软件，<strong>在不打开网站内容页面的情况下阅读支持RSS输出的网站内容</strong>。</p>
</blockquote>
<p>说了这么多，你可以简单的把它理解为<strong>把多个平台的内容汇聚在一起，让你能在一个平台上看到</strong></p>
<h1 id="为什么"><a href="#为什么" class="headerlink" title="为什么"></a>为什么</h1><p>如果你关注了多个博主（信号源），每天都要看一遍他们有没有更新，要奔波于各个网站，应用之间。RSS把这些内容汇聚到了一起，让你在一个平台上观看，还能避免博客平台的“垃圾信息和广告”，让你有个安静的阅读环境，我想这就是RSS的意义</p>
<h1 id="怎么用"><a href="#怎么用" class="headerlink" title="怎么用"></a>怎么用</h1><h2 id="对于有RSS功能的网页"><a href="#对于有RSS功能的网页" class="headerlink" title="对于有RSS功能的网页"></a>对于有RSS功能的网页</h2><p>建议使用<a href="https://www.qireader.com.cn/">QiReader</a>，这是一个界面现代的RSS阅读器，进入官网后点击注册，注册账号后点击添加内容，在输入框内输入-点击RSS按钮后出现的网页的网址链接-，直接复制过去回车，点击订阅即可<br><img src="https://github.com/user-attachments/assets/be41f273-7bb6-4fb7-89f8-9047d977142d" alt="屏幕截图 2024-07-25 154933"></p>
<h2 id="对于无RSS功能的网页"><a href="#对于无RSS功能的网页" class="headerlink" title="对于无RSS功能的网页"></a>对于无RSS功能的网页</h2><p>建议使用<a href="https://docs.rsshub.app/zh/">RSSHub</a>和<a href="https://github.com/yang991178/fluent-reader">Fluent-Reader</a></p>
<ol>
<li>进入<a href="https://github.com/yang991178/fluent-reader">Fluent-Reader</a>后，记得先star支持下作者。点击Releases下的第一个按钮，选择适合自己电脑的版本，点击下载。</li>
<li>进入<a href="https://docs.rsshub.app/zh/guide/">RSSHub</a>，点击“食用指南”，在左侧找到想订阅的博客类型，在右侧寻找博客，再在博客的细分类中找到自己要订阅的源，点击。</li>
<li>我以-论坛-百度-精品帖子-为例，点击后复制Example后的链接，链接后有一个复制按钮，点击以复制。</li>
<li>打开Fluent-Reader，在右上角设置-添加订阅源-栏里粘贴复制的链接，<strong>不要点击“添加”</strong></li>
<li><strong>重要的一步</strong>，在RSSHub中打开“公共实例”，在Public表中找到一个可用“Online列为up”的URL，选中复制</li>
<li>回到Fluent-Reader，把填进去的链接中“rsshub.app”替换成你复制的链接，点击添加，大功告成！<br><img src="https://github.com/user-attachments/assets/4743e26f-4829-47de-bd0a-7a57146dd8ab" alt="屏幕截图 2024-07-26 134635"><blockquote>
<p><a href="https://www.bilibili.com/video/BV1mz42197g1/">视频版教程</a><br>但凡在Github下载过东西的都知道它下载的有多慢，我把Fluent-Reader的下载程序放到网盘里了，需要自取<a href="https://www.123pan.com/s/hoPKVv-2aB63.html">123网盘</a><br>其实使用<a href="https://docs.rsshub.app/zh/">RSSHub</a>和<a href="https://www.qireader.com.cn/">QiReader</a>搭配也可以，甚至不用更换“rsshub.app”了（应该），对于有RSS功能的网页，也可以使用Fluent-Reader。但是QiReader功能较少，但方便。可以选择自己喜欢的。</p>
</blockquote>
</li>
</ol>
]]></content>
      <tags>
        <tag>科普</tag>
        <tag>如何____</tag>
      </tags>
  </entry>
  <entry>
    <title>2024年7月科技新闻亮点</title>
    <url>/2024/07/30/2024%E5%B9%B47%E6%9C%88%E7%A7%91%E6%8A%80%E6%96%B0%E9%97%BB%E4%BA%AE%E7%82%B9/</url>
    <content><![CDATA[<p>随着技术的飞速发展，2024年7月见证了数码设备和人工智能领域的一些令人兴奋的进展。从新的移动设备到人工智能技术的应用，这个月充满了创新和惊喜。让我们一起来看看本月的科技新闻亮点。</p>
<h3 id="数码设备"><a href="#数码设备" class="headerlink" title="数码设备"></a>数码设备</h3><h4 id="小米新移动固态硬盘"><a href="#小米新移动固态硬盘" class="headerlink" title="小米新移动固态硬盘"></a><strong>小米新移动固态硬盘</strong></h4><p>小米推出了一款新移动固态硬盘，提供了1TB的存储空间，读写速度达到了惊人的2000MB&#x2F;s。这款SSD不仅便携，而且性能强劲，非常适合需要快速数据传输的专业人士。</p>
<h4 id="蔚来第二代-NIO-Phone"><a href="#蔚来第二代-NIO-Phone" class="headerlink" title="蔚来第二代 NIO Phone"></a><strong>蔚来第二代 NIO Phone</strong></h4><p>电动汽车制造商蔚来发布了其第二代NIO Phone。虽然具体配置尚未公布，但这款手机预计会集成一些与蔚来汽车生态系统相关的独特功能，比如远程控制和车辆状态监控等。</p>
<h4 id="索尼-A6000-微单数码相机"><a href="#索尼-A6000-微单数码相机" class="headerlink" title="索尼 A6000 微单数码相机"></a><strong>索尼 A6000 微单数码相机</strong></h4><p>索尼更新了A6000微单相机，推出了一款纯白色的新版本。新款相机不仅外观时尚，而且具备出色的成像质量和快速的自动对焦系统，非常适合摄影爱好者。</p>
<h4 id="七彩虹源笔记本-N14"><a href="#七彩虹源笔记本-N14" class="headerlink" title="七彩虹源笔记本 N14"></a><strong>七彩虹源笔记本 N14</strong></h4><p>七彩虹源发布了新款N14笔记本电脑，顶配版搭载第13代Intel i9处理器与NVIDIA RTX 4070显卡。这款笔记本电脑为高性能计算和图形处理提供了强大的硬件支持。</p>
<h4 id="vivo-Y37-手机"><a href="#vivo-Y37-手机" class="headerlink" title="vivo Y37 手机"></a><strong>vivo Y37 手机</strong></h4><p>vivo推出了Y37手机，搭载联发科天玑系列芯片，定位于中端市场，以其优秀的性价比吸引了消费者的关注。</p>
<h3 id="人工智能"><a href="#人工智能" class="headerlink" title="人工智能"></a>人工智能</h3><h4 id="OpenAI-的-SearchGPT"><a href="#OpenAI-的-SearchGPT" class="headerlink" title="OpenAI 的 SearchGPT"></a><strong>OpenAI 的 SearchGPT</strong></h4><p>OpenAI推出了名为SearchGPT的在线搜索工具，通过AI技术提供更加个性化和准确的搜索结果。这款工具可能会给搜索引擎市场带来新的变化，挑战现有的领导者谷歌。</p>
<h4 id="汉王科技的人工智能产品"><a href="#汉王科技的人工智能产品" class="headerlink" title="汉王科技的人工智能产品"></a><strong>汉王科技的人工智能产品</strong></h4><p>汉王科技继续推出一系列人工智能产品，涵盖仿生科技、智慧学习、智能办公和数字绘画等多个领域。虽然具体型号没有提及，但可以期待这些产品在各自的细分市场上有所突破。</p>
<h3 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h3><p>2024年7月对于科技爱好者来说无疑是一个充满期待的月份。无论是全新的数码设备还是人工智能技术的进步，都让我们看到了未来科技发展的无限可能。随着这些新技术的普及和发展，我们期待更多的创新产品和服务能够改变我们的生活。</p>
]]></content>
      <tags>
        <tag>新闻</tag>
        <tag>AI纪元</tag>
      </tags>
  </entry>
  <entry>
    <title>勇气：雷军与小米的非凡旅程</title>
    <url>/2024/07/20/%E5%8B%87%E6%B0%94%EF%BC%9A%E9%9B%B7%E5%86%9B%E4%B8%8E%E5%B0%8F%E7%B1%B3%E7%9A%84%E9%9D%9E%E5%87%A1%E6%97%85%E7%A8%8B/</url>
    <content><![CDATA[<p><img src="https://github.com/user-attachments/assets/46e30fa9-b528-4db3-8fb1-b8a2b7aff261" alt="92f8fca6efd5c045852b447894905056eecf72f9 3"></p>
<p>在科技行业，每一次跨越都是一次勇气的考验。2024年的盛夏，小米集团的创始人、董事长兼CEO雷军，站在一年一度的个人演讲舞台上，用“勇气”二字串联起了过去与未来，科技与梦想，以及个人与品牌的壮丽篇章。</p>
<h2 id="小米汽车：从零到一的勇气"><a href="#小米汽车：从零到一的勇气" class="headerlink" title="小米汽车：从零到一的勇气"></a>小米汽车：从零到一的勇气</h2><p>雷军的演讲开篇，就带我们进入了小米汽车的奇妙世界。自2021年宣布进军汽车行业以来，小米汽车成为了业界关注的焦点。雷军坦诚分享了这一路上的艰难与坚持。面对外界的质疑，小米选择了不走寻常路——拒绝拆分独立融资，而是将汽车项目置于集团内部，以小米自身的资金与资源全力推进。这一决策背后，是对未来趋势的洞察，更是对自身能力的信任。</p>
<p>雷军本人，作为一位企业家，更是一位勇敢的探索者。他不仅亲自试驾了170余款汽车，还考取了赛车驾照，以实际行动诠释了对汽车行业的热爱与尊重。这一系列举动，不仅加深了他对于汽车行业的理解，也向世人展示了小米汽车项目的决心与信心。</p>
<h2 id="破局者：投资与自我超越"><a href="#破局者：投资与自我超越" class="headerlink" title="破局者：投资与自我超越"></a>破局者：投资与自我超越</h2><p>在市场波动的背景下，小米的股价也曾经历过起伏。面对挑战，雷军领导的小米采取了果断措施——回购股票，展现了管理层对于公司长期价值的坚定信念。这一策略不仅稳定了市场预期，也彰显了小米对于自我价值的清晰认知。</p>
<p>更重要的是，小米始终将“最好的投资就是投资自己”作为行动指南。无论是产品创新，还是品牌建设，小米都在不断突破自我，用品质与服务赢得用户的信任。这一过程中，小米不仅破除了外界对于品牌形象的固有认知，更是在全球市场上树立起了一面旗帜，成为科技行业的破局者。</p>
<h2 id="科技之光：新品发布与未来愿景"><a href="#科技之光：新品发布与未来愿景" class="headerlink" title="科技之光：新品发布与未来愿景"></a>科技之光：新品发布与未来愿景</h2><p>演讲的高潮，自然落在了小米的最新产品上。小米MIX Fold4与小米MIX Flip两款折叠屏手机的发布，标志着小米在移动通讯领域的又一次飞跃。小米MIX Fold4凭借其卓越的屏幕技术、极致的性能和优雅的设计，再次定义了折叠屏手机的新标准；而小米MIX Flip，则以小巧轻盈的形态，开辟了折叠屏手机的全新可能性，满足了用户对于时尚与实用性的双重需求。</p>
<p>除此之外，雷军还揭秘了一款概念产品——小米SU7 Ultra Prototype，这款产品集成了小米最前沿的科技成果，预示着小米在高科技领域的持续探索与创新。每一款新品的背后，都是小米对未来的憧憬与追求，它们共同编织出一幅关于科技与生活的美好图景。</p>
<h2 id="个人成长与勇气的赞歌"><a href="#个人成长与勇气的赞歌" class="headerlink" title="个人成长与勇气的赞歌"></a>个人成长与勇气的赞歌</h2><p>演讲尾声，雷军回归初心，分享了个人成长的故事。从金山软件的风雨岁月，到小米的辉煌征程，每一步都充满了挑战与机遇。雷军的赛车驾照与漂移视频，不仅仅是一次速度与激情的体验，更是勇气与梦想的见证。它们告诉我们，无论前方的路有多么崎岖，只要有勇气，就有无限可能。</p>
<p>雷军的年度演讲，不仅是一场科技盛宴，更是一次心灵的触动。它让我们看到了一个企业从无到有、从弱到强的成长轨迹，也让我们感受到了一个领导者对于梦想的执着追求。在未来的日子里，小米将继续以勇气为帆，驶向更加广阔的科技蓝海，书写属于自己的传奇篇章！</p>
]]></content>
      <tags>
        <tag>发布会</tag>
      </tags>
  </entry>
  <entry>
    <title>告别云担忧：手把手教你本地部署AI大模型</title>
    <url>/2024/07/23/%E5%91%8A%E5%88%AB%E4%BA%91%E6%8B%85%E5%BF%A7%EF%BC%9A%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2AI%E5%A4%A7%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>当我们想跟AI对话时，把一些文件发送到云端可能会不安全，这时候，就需要本地部署了。本地部署指的是AI会调用电脑本地的算力进行计算，所以，电脑的性能不可以太差。下面是本地部署大模型的教程。</p>
<blockquote>
<p>请务必<strong>看完全文后再进行操作</strong><br>本教程使用Win10，如果您是Mac用户，请参考<a href="https://www.bilibili.com/video/BV18E421N7d4/?vd_source=4d615fafe7d9dfc43c90bfcb3b9fdcbb">Mac本地部署教程</a></p>
</blockquote>
<h2 id="教程"><a href="#教程" class="headerlink" title="教程"></a>教程</h2><h3 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h3><p>下载<a href="https://ollama.com/">Ollama</a>,进入主页点击Download，选择自己的操作系统，点击下载</p>
<blockquote>
<p>您无需点击下方的注册按钮</p>
</blockquote>
<h3 id="下载大模型"><a href="#下载大模型" class="headerlink" title="下载大模型"></a>下载大模型</h3><p>1.下载完成后点击Win+R，“打开”栏输入cmd，回车<br>2.打开 <a href="https://ollama.com/">Ollama官网</a>，点击右上角的 <code>Search models</code>，输入你想下载的开源模型名称。找到模型后，复制页面中提供的下载命令到CMD窗口，回车开始下载。<br><img src="https://github.com/user-attachments/assets/22fdeeeb-6382-4051-8164-e4b800d99b62" alt="屏幕截图 2024-07-23 102305"><br><img src="https://github.com/user-attachments/assets/45c070d8-c8e4-4d37-8712-442505c366ed" alt="屏幕截图 2024-07-23 103510"></p>
<blockquote>
<p>每个模型的代码不一样，上图使用Qwen2-72b演示，请根据电脑性能选择下载的模型。一般来说，7b左右的模型适合大部分的电脑，我2016年的显卡都带的动。72b左右的模型则需要强大性能的电脑，它的安装包就足有40GB！<br>建议选择qwen2，因为它是迄今为止<strong>最强大的开源大模型</strong>，具体可以看<a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard">🤗 Open LLM Leaderboard</a>，您也可以选择自己喜欢的大模型，方法相同<br><img src="https://github.com/user-attachments/assets/bdd3cc45-c8e0-4c89-afa9-9e57c6ed8773" alt="屏幕截图 2024-07-23 104504"></p>
</blockquote>
<h3 id="使用大模型"><a href="#使用大模型" class="headerlink" title="使用大模型"></a>使用大模型</h3><p>完成下载后，你将看到类似以下界面，这意味着一切准备就绪！<br><img src="https://github.com/user-attachments/assets/6e1cccf9-3def-48e2-97a3-270844a1e946" alt="屏幕截图 2024-07-23 105346"><br>测试一下<br><img src="https://github.com/user-attachments/assets/4ab8d016-3abe-4c05-a826-e37cdb8a3596" alt="屏幕截图 2024-07-23 110142"></p>
<blockquote>
<p>要再次使用模型，重复上述<strong>下载大模型的第1-2步</strong>，但这次回车后将启动而不是重新下载模型。请确保Ollama程序正在运行。<br>由于Ollama位于国外，国内用户可能会遇到下载速度缓慢的问题。为此，我已将下载链接上传至网盘，你可以直接从那里下载，无需注册。<br><a href="https://www.123pan.com/s/hoPKVv-RMB63.html">123网盘下载</a><br>请忽略下载页的广告，解压缩后下载</p>
</blockquote>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>什么，界面不好看？下期博客教大家获得更好地UI界面</p>
]]></content>
      <tags>
        <tag>如何____</tag>
        <tag>AI纪元</tag>
        <tag>折腾</tag>
      </tags>
  </entry>
  <entry>
    <title>如何流畅访问Github</title>
    <url>/2024/08/10/%E5%A6%82%E4%BD%95%E6%B5%81%E7%95%85%E8%AE%BF%E9%97%AEGithub/</url>
    <content><![CDATA[<blockquote>
<p>水的一篇文，下周旅游更没时间写</p>
</blockquote>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>众所周知，Github是全球最大的<strong>同性交友平台</strong>，但还是众所周知，Github现在是处于一个半墙不墙的状态，有用户访问会特别快，有用户压根进不去，本篇博客教大家如何流畅访问Github！</p>
<h2 id="教程"><a href="#教程" class="headerlink" title="教程"></a>教程</h2><p>打开微软应用商店，搜索Watt Toolkit，这是大名鼎鼎的Steam++，只不过是改名了。这是一个开源软件，你可以去<a href="https://github.com/liushiyangFrsan/GitHub_WattToolkit">Github</a>查看，并且此软件上架了微软应用商店，安全性不必担心，但仍然不建议你打开任何敏感网站，这可能是<strong>触犯法律的</strong><br>下载完成后找到网络加速，勾选需要加速的网站，点击一键加速即可<br>这个软件还能加速其它网站，比如Steam游戏商店，各大国外网盘等</p>
<h2 id="合法性"><a href="#合法性" class="headerlink" title="合法性"></a>合法性</h2><p>这是大家最关注的问题，我问了问AI</p>
<hr>
<p>反向代理是指服务器代理网络上的客户机请求，将请求转达给内部真实服务器，然后将结果返回给 Internet 客户端，代理服务器上没有任何网页资料。其工作原理是当用户浏览器发出一个 HTTP 请求时，通过域名解析将请求定向到反向代理服务器（如果要实现多个 WEB 服务器的反向代理，需要将多个 WEB 服务器的域名都指向反向代理服务器），由反向代理服务器处理器请求。反向代理一般只缓存可缓冲的数据（比如 html 网页和图片等），而一些 CGI 脚本程序或者 ASP 之类的程序不缓存，它根据从 WEB 服务器返回的 HTTP 头标记来缓冲静态页面。</p>
<p>关于使用本地反向代理软件是否合法，需要根据具体情况来判断。如果使用反向代理软件的目的是为了绕过封锁限制访问的行为，则属于违法行为。根据《中华人民共和国计算机信息网络国际联网管理暂行规定》第六条规定，计算机信息网络直接进行国际联网，必须使用邮电部国家公用电信网提供的国际出入口信道。任何单位和个人不得自行建立或者使用其他信道进行国际联网。<strong>如果是为了提高网络访问速度、增强网络安全性等合法目的，并且遵守相关的法律法规和服务条款，那么使用反向代理软件是合法的。</strong></p>
<p>总的来说，在使用反向代理软件时，建议仔细阅读软件的使用说明和相关的法律法规，以确保自己的行为合法合规。</p>
<hr>
<p><del>我觉得还是魔法好用</del></p>
]]></content>
      <tags>
        <tag>科普</tag>
        <tag>如何____</tag>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title>如何让本地部署的大模型有个优雅的界面</title>
    <url>/2024/07/31/%E5%A6%82%E4%BD%95%E8%AE%A9%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%89%E4%B8%AA%E4%BC%98%E9%9B%85%E7%9A%84%E7%95%8C%E9%9D%A2/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>有人可能受不了本地大模型这样辣鸡的聊天界面，本期博客教大家如何拥有一个<strong>优雅的聊天界面</strong><br><img src="https://github.com/user-attachments/assets/e584e938-f8d2-404a-b582-df752bbd5460" alt="4ab8d016-3abe-4c05-a826-e37cdb8a3596"></p>
<h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>使用<a href="https://www.docker.com/">Docker</a>+<a href="https://github.com/open-webui/docs">OpenWebUI</a>来进行搭建</p>
<h2 id="开始搭建"><a href="#开始搭建" class="headerlink" title="开始搭建"></a>开始搭建</h2><p>1.打开Docker官网，选择自己的电脑系统，点击下载。<br>2.打开OpenWebUI，找到About栏下方的链接，点击后找到GettingStarted→Installation with Default Configuration→If Ollama is on your computer, use this command，复制下面的代码<br>3.此时Docker已经下载完成，进入Docker（无需注册）<br>4.打开cmd窗口，输入复制的代码，所有内容下载完成后关闭<br>5.此时Docker的窗口已经出现了OpenWebUI，点击Port栏下面的端口即可打开聊天界面<br>6.注册一个账号，第一个注册的账号就是管理员账号<br>7.此时已经大功告成<br><img src="https://github.com/user-attachments/assets/5f638dd8-6759-4b89-861a-d8d8a9ace711" alt="image"></p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>Docker地下载花了我3个多小时，为此，我把下载链接放到网盘里了，需要自取<br><a href="https://peterwu.lanzout.com/iv7nh262gohi">蓝奏云</a></p>
]]></content>
      <tags>
        <tag>如何____</tag>
        <tag>AI纪元</tag>
        <tag>折腾</tag>
      </tags>
  </entry>
  <entry>
    <title>开源模型比闭源强？Llama3.1-405b发布！</title>
    <url>/2024/07/26/%E5%BC%80%E6%BA%90%E6%A8%A1%E5%9E%8B%E6%AF%94%E9%97%AD%E6%BA%90%E5%BC%BA%EF%BC%9FLlama3.1-405b%E5%8F%91%E5%B8%83%EF%BC%81/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>前天，MetaAI发布了超大尺寸的开源模型——Llama3.1-405b，使用Ollama就可以部署，教程参照上上期博客，在cmd窗口输入部署命令即可</p>
<h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><table>
<thead>
<tr>
<th>模型尺寸</th>
<th>模型大小</th>
<th>部署命令</th>
</tr>
</thead>
<tbody><tr>
<td>llama3.1-8b</td>
<td>4.7GB</td>
<td><code>ollama run llama3.1:8b</code></td>
</tr>
<tr>
<td>llama3.1-70b</td>
<td>40GB</td>
<td><code>ollama run llama3.1:70b</code></td>
</tr>
<tr>
<td>llama3.1-405b</td>
<td>231GB</td>
<td><code>ollama run llama3.1:405b</code></td>
</tr>
</tbody></table>
<h2 id="官方描述"><a href="#官方描述" class="headerlink" title="官方描述"></a>官方描述</h2><h3 id="Meta-Llama-3-1"><a href="#Meta-Llama-3-1" class="headerlink" title="Meta Llama 3.1"></a>Meta Llama 3.1</h3><p><img src="https://github.com/user-attachments/assets/2909eed4-fa28-4e65-a0c4-8758bb5a213d" alt="4d0cab8e-952b-4c75-b110-1514d8db8fae"><br><strong>Llama 3.1</strong>模型系列可用： </p>
<ul>
<li><strong>8B</strong></li>
<li><strong>70B</strong></li>
<li><strong>405B</strong></li>
</ul>
<p>Llama 3.1 405B 是首个公开可用的模型，在常识、可操控性、数学、工具使用和多语言翻译等方面的先进能力上可与顶级人工智能模型相媲美。<br>8B 和 70B 模型的升级版本是多语言的，并且具有 128K 的显著更长的上下文长度、先进的工具使用能力和更强的整体推理能力。这使得 Meta 的最新模型能够支持高级用例，例如长篇文本摘要、多语言会话代理和编码助手。<br>Meta 还对其许可证进行了更改，允许开发人员使用 Llama 模型（包括 405B 模型）的输出以改进其他模型。</p>
<p>对于此次发布，Meta 已在涵盖多种语言的 150 多个基准数据集上评估了性能。此外，Meta 进行了广泛的人工评估，在现实场景中将 Llama 3.1 与竞争模型进行了比较。Meta 的实验评估表明，我们的旗舰模型在一系列任务上与领先的基础模型（包括 GPT-4、GPT-4o 和 Claude 3.5 Sonnet）具有竞争力。此外，Meta 的较小模型与具有相似参数数量的封闭和开放模型具有竞争力。 </p>
<h4 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h4><p>对于此次发布，Meta 已在超过 150 个涵盖多种语言的基准数据集上评估了性能。此外，Meta 进行了广泛的人工评估，在现实场景中将 Llama 3.1 与竞争模型进行了比较。Meta 的实验评估表明，我们的旗舰模型在包括 GPT-4、GPT-4o 和 Claude 3.5 Sonnet 在内的一系列任务中与领先的基础模型具有竞争力。此外，Meta 的较小模型与具有相似参数数量的封闭和开放模型具有竞争力。<br><img src="https://github.com/user-attachments/assets/77ba3ac9-9290-4cb3-ba87-858cabce9bee" alt="73b11a5e-84e9-4397-9d47-f0299a6294b3"></p>
<p><img src="https://github.com/user-attachments/assets/e75105ca-ae37-46d3-a64b-e8c09ea3fa35" alt="ad042a1c-bbc7-47de-bbbf-78a3cfc13485"></p>
<p><img src="https://github.com/user-attachments/assets/52dc29e7-29dc-4a65-994b-34be420eef14" alt="2d582df5-ce45-4326-85c5-254c917554b2"></p>
<h4 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h4><ul>
<li><a href="https://ai.meta.com/blog/meta-llama-3-1/">Meta AI Llama 3.1 launch blog post</a></li>
</ul>
<p>由<strong>豆包</strong>翻译</p>
<hr>
<p>什么，你想说你要在自己电脑上部署405b，很遗憾，<strong>你的电脑几乎不支持运转如此大规格的模型</strong>，你可以继续玩上次部署的Qwen2<br>我问了问通义千问，这是它说的<br>Ask：模型尺寸为405b的开源模型需要怎样的电脑可以部署<br>Answer：对于一个拥有4050亿参数的大规模语言模型，如Meta即将发布的Llama 3-405B模型，部署这样的模型需要非常强大的计算资源。以下是一些推荐的硬件配置要求：</p>
<p>CPU<br>处理器: 高性能多核心处理器，例如Intel Xeon或AMD EPYC系列。这些处理器通常具有较多的核心数和较高的时钟速度，适合处理并行计算任务。<br>核心数量: 至少16核心，32核心或更多将更为理想。<br>缓存: 大量的三级缓存有助于提高性能。<br>GPU<br>显卡: 高端的专业级GPU，例如NVIDIA A100、H100或类似的高性能GPU。<br>显存: 至少32GB或更高，考虑到模型的大小，可能需要更大的显存。<br>CUDA核心数量: 越多越好，以支持并行处理能力。<br>内存 (RAM)<br>系统内存: 至少128GB或更高，以便能够存储模型和其他相关数据。<br>存储<br>SSD: 快速NVMe SSD用于操作系统和应用程序安装。<br>额外存储: 可能需要TB级别的存储空间来存放模型文件和其他相关数据。<br>其他组件<br>电源供应器 (PSU): 高功率且高效的电源供应器，至少850W以上。<br>冷却系统: 高效的散热解决方案，包括水冷系统。<br>主板: 支持所需CPU和GPU的高端主板，带有足够的PCI-E插槽。<br>网络连接<br>高速网络接口: 为了数据传输，可能需要10Gbps或更快的网络接口卡。<br>软件环境<br>操作系统: Linux发行版，比如Ubuntu Server，因为大多数深度学习框架都针对Linux进行了优化。<br>编程框架: PyTorch或其他支持大规模模型的框架。<br>驱动程序: 最新的GPU驱动程序以获得最佳性能。<br>请注意，<strong>上述配置是非常高端的，并且成本极高。部署这样规模的模型通常是在数据中心或云环境中进行的，而不是个人电脑。如果是在本地部署，那么这将是一个相当昂贵的投资。</strong>另外，由于模型的复杂性和计算需求，可能还需要分布式计算技术来分摊处理负载。</p>
<h2 id="在线体验"><a href="#在线体验" class="headerlink" title="在线体验"></a>在线体验</h2><p>虽然不能本地部署，但可以线上体验呀（都需要魔法）</p>
<h3 id="方式1：优先使用HuggingFace"><a href="#方式1：优先使用HuggingFace" class="headerlink" title="方式1：优先使用HuggingFace"></a>方式1：优先使用<a href="https://huggingface.co/">HuggingFace</a></h3><p>进入主页往下滑，找到“Trending on this week”→“Spaces”→“Llama3.1-405B”，点击后无需注册即可聊天，需要先调教下让它说中文。虽然加载进页面慢，但回复真的炒鸡快！<br><img src="https://github.com/user-attachments/assets/20099c67-9d6d-4b76-9eeb-50913a27405f" alt="屏幕截图_26-7-2024_17583_huggingface co"></p>
<h3 id="方式2：使用MetaAI"><a href="#方式2：使用MetaAI" class="headerlink" title="方式2：使用MetaAI"></a>方式2：使用<a href="https://www.meta.ai/">MetaAI</a></h3><p>注册后即可使用</p>
]]></content>
      <tags>
        <tag>AI纪元</tag>
      </tags>
  </entry>
</search>
